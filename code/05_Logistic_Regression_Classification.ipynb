{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 05 Text Classification with LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>plot</th>\n",
       "      <th>is_indian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0 Earthquake</td>\n",
       "      <td>As a series of minor earthquakes start tearing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12 Rounds (film)</td>\n",
       "      <td>A sting operation to capture arms dealer Miles...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12 Rounds 3: Lockdown</td>\n",
       "      <td>Detective Tyler Burke  and his two men infiltr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200 mph</td>\n",
       "      <td>When the older brother (Tommy Nash) he idolize...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ablaze (2001 film)</td>\n",
       "      <td>Andrew Thomas is an agent tasked with recordin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>Choked (film)</td>\n",
       "      <td>Sarita Pillai and Sushant Pillai are a married...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>Chumbak</td>\n",
       "      <td>Chumbak is a coming-of-age story of Baalu, a t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>Chungakkarum Veshyakalum</td>\n",
       "      <td>Chungakkarum Veshyakalum is the story of a Mal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>Chuzhi</td>\n",
       "      <td>Varghese is a planter who lives with his wife ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>Cinema Paithiyam</td>\n",
       "      <td>Jaya is a cinephile. So obsessed is she with c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         title  \\\n",
       "0              10.0 Earthquake   \n",
       "1             12 Rounds (film)   \n",
       "2        12 Rounds 3: Lockdown   \n",
       "3                      200 mph   \n",
       "4           Ablaze (2001 film)   \n",
       "...                        ...   \n",
       "3995             Choked (film)   \n",
       "3996                   Chumbak   \n",
       "3997  Chungakkarum Veshyakalum   \n",
       "3998                    Chuzhi   \n",
       "3999          Cinema Paithiyam   \n",
       "\n",
       "                                                   plot  is_indian  \n",
       "0     As a series of minor earthquakes start tearing...          0  \n",
       "1     A sting operation to capture arms dealer Miles...          0  \n",
       "2     Detective Tyler Burke  and his two men infiltr...          0  \n",
       "3     When the older brother (Tommy Nash) he idolize...          0  \n",
       "4     Andrew Thomas is an agent tasked with recordin...          0  \n",
       "...                                                 ...        ...  \n",
       "3995  Sarita Pillai and Sushant Pillai are a married...          1  \n",
       "3996  Chumbak is a coming-of-age story of Baalu, a t...          1  \n",
       "3997  Chungakkarum Veshyakalum is the story of a Mal...          1  \n",
       "3998  Varghese is a planter who lives with his wife ...          1  \n",
       "3999  Jaya is a cinephile. So obsessed is she with c...          1  \n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/merged_plots.csv', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter_stemmer=PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "non_name_words = set(nltk.corpus.words.words())\n",
    "\n",
    "def clean(text, ner=False):\n",
    "\n",
    "    # lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # removing special characters\n",
    "    text = re.sub('\\\\W', ' ', text)\n",
    "\n",
    "    # splitting into tokens\n",
    "    words = text.split()\n",
    "\n",
    "    # removing stopwords\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        if not word in stop_words:\n",
    "            filtered_words.append(word)\n",
    "    \n",
    "    # word stemming\n",
    "    stemmed_words = [porter_stemmer.stem(word) for word in filtered_words]\n",
    "    \n",
    "    # removing names\n",
    "    if ner:\n",
    "        words_with_ner = []\n",
    "        for word in stemmed_words:\n",
    "            if word in non_name_words:\n",
    "                words_with_ner.append(word)\n",
    "        result = words_with_ner\n",
    "    else:\n",
    "        result = stemmed_words\n",
    "        \n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Named Entity Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seri minor earthquak start tear apart lo angel scientist emili usg theoriz build super quak drop entir citi lava fill chasm engin jack whose daughter gone camp friend danger area whose compani respons quak due deep frack feel oblig help race emili increasingli damag citi hope divert epicent long beach potenti save million live citi lo angel\n"
     ]
    }
   ],
   "source": [
    "# NamedEntityRemoval = False\n",
    "plots_cleaned = [clean(plot, ner=False) for plot in df['plot']]\n",
    "print(plots_cleaned[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Named Entity Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minor start tear apart lo angel scientist build super drop lava fill chasm jack whose daughter gone camp friend danger area whose due deep frack feel help race hope divert long beach save million live lo angel\n"
     ]
    }
   ],
   "source": [
    "# NamedEntityRemoval = True\n",
    "plots_cleaned_ner = [clean(plot, ner=True) for plot in df['plot']]\n",
    "print(plots_cleaned_ner[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "Scikit-learn’s CountVectorizer is used to convert a collection of text documents to a vector of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length WITHOUT NER: 32340\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=0)\n",
    "vectorizer.fit(plots_cleaned)\n",
    "print('Vocab length WITHOUT NER:', len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length WITH NER: 8867\n"
     ]
    }
   ],
   "source": [
    "vectorizer_ner = CountVectorizer(min_df=0)\n",
    "vectorizer_ner.fit(plots_cleaned_ner)\n",
    "print('Vocab length WITH NER:', len(vectorizer_ner.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['is_indian'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = vectorizer.transform(plots_cleaned)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state= 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ner = vectorizer_ner.transform(plots_cleaned_ner)\n",
    "\n",
    "X_train_ner, X_test_ner, y_train, y_test = train_test_split(X_ner, y, test_size=0.25, random_state= 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reg = LogisticRegression(solver='liblinear', random_state=42)\n",
    "reg = reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ner = LogisticRegression(solver='liblinear', random_state=42)\n",
    "reg_ner = reg_ner.fit(X_train_ner, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy sore WITHOUT ner: 0.938\n"
     ]
    }
   ],
   "source": [
    "predictions = reg.predict(X_test)\n",
    "score = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(f'Accuracy sore WITHOUT ner: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy sore WITH ner: 0.916\n"
     ]
    }
   ],
   "source": [
    "predictions_ner = reg_ner.predict(X_test_ner)\n",
    "score_ner = accuracy_score(y_test, predictions_ner)\n",
    "\n",
    "print(f'Accuracy sore WITH ner: {score_ner}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "69a9235b2799f09bc7a4d7fc4018927df298a0a697379818c8dec9478f72590e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
